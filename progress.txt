## Reminder

**Examples of good progress.txt additions:**
- "When modifying X, also update Y to keep them in sync"
- "This module uses pattern Z for all API calls"
- "Tests require the dev server running on PORT 3000"
- "Field names must match the template exactly"

**Do NOT add:**
- Story-specific implementation details
- Temporary debugging notes
- Information already in progress.txt

---
## Codebase Patterns
- Kafka client functions in `kafka_client.py` catch all exceptions internally, log warnings, return empty/zero values — never raise
- Module imports inside functions (e.g., `from kafka_client import get_committed_offsets` inside `_process_group`) require patching at the source module, not the calling module
- Use `uv pip` instead of `pip`, `uv run python` instead of `python`
- SQLite PRAGMA `auto_vacuum=INCREMENTAL` must be set BEFORE creating tables and requires a `commit()` to take effect
- Each worker thread (Sampler, Reporter, Housekeeping, StateManager) should receive `db_path` string and create its own connection via `database.get_connection(db_path)` - never share a connection across threads
- Tests that use StateManager/Reporter/Housekeeping need initialized database; use `db_path_initialized` fixture from conftest.py
- When calling `get_committed_offsets` from kafka_client, convert Set to List since the function expects List[Tuple[str, int]]

---
## Learnings
Kafka / Distributed Systems

When a function creates a resource that's needed later, return it for reuse rather than creating another instance
The describe_consumer_groups call already provides per-group partition data — don't discard it
Idle group detection relies on per-group partition assignments; groups with no active assignments properly return empty partition sets

Testing / Patching

When patching function imports inside methods, patch at the source module (e.g. kafka_client.get_committed_offsets) not the calling module
Tests that create workers (StateManager, Reporter, Housekeeping) need an initialized database — use a db_path_initialized fixture
Fix test fixtures to use db_path instead of passing connections directly

SQLite

PRAGMA auto_vacuum must be set BEFORE creating tables and requires an explicit commit
PRAGMA statements don't support bound parameters — validate input first, then use an f-string
When you only need one value from a table, write a purpose-built query rather than fetching all rows and extracting one

Worker / Threading Patterns

SQLite connections should be per-thread — pass db_path and let each worker create its own connection
For timed worker loops: capture cycle_start, run work, calculate elapsed, subtract from interval, sleep max(0, remainder) — prevents back-to-back cycles when work exceeds the interval

State / Business Logic

New groups start with consecutive_static=0 and will report ONLINE for threshold cycles before being detectable as OFFLINE — this is a known startup blind spot, not a bug
Groups with DB history are identified by presence in the consumer_commits table
When marking idle groups as OFFLINE, iterate over all previously tracked topics for that group
RECOVERING groups are actively consuming, so they warrant "fine" resolution — only OFFLINE should produce "coarse" resolution


---
## Progress updates

## TASK 32 — INCOMPLETE: partition_offsets stale for sole-consumer idle groups
Status: Partially fixed. The shared-topic case is resolved. The sole-consumer case is not.
The fix correctly augments all_topic_partitions with idle groups' historical partitions, ensuring get_latest_produced_offsets fetches a fresh high-watermark for those partitions. This is the right first step.
The gap: _write_partition_offset_if_needed is only called from inside _process_group, which only iterates over active_groups. When apple is the sole consumer of test/0, no active group processes that partition — so latest_offsets[("test", 0)] is fetched correctly but immediately discarded. Nobody writes it to partition_offsets. The table remains frozen at the pre-stop value. The reporter still sees committed_offset >= newest_stored_offset and returns (0, "current").
The fix works in the shared-topic case only: if banana or cat also consume test/0, their _process_group execution will call _write_partition_offset_if_needed("test", 0, ...) and the fresh offset is persisted. But apple as sole consumer — the exact scenario you reported — is still broken.
How to fix:
The augmentation block already knows precisely which partitions belong to idle groups — it built that set iteratively. The fix needs to track those idle-group partitions separately (not just merge them into all_topic_partitions) so that after get_latest_produced_offsets returns, a dedicated write-pass can be made for them.
Conceptually, the run loop needs a second set — call it idle_topic_partitions — populated during the augmentation loop instead of merging into all_topic_partitions directly. Both sets are merged for the Kafka fetch (so the bulk call is unchanged), but after latest_offsets comes back, the code iterates idle_topic_partitions and calls _write_partition_offset_if_needed for each one using the fetched offset from latest_offsets. This write pass sits between Step 3 and Step 4 (before the active-group processing loop), and its writes are covered by the existing commit_batch at the end of the cycle.
_write_partition_offset_if_needed already has all the right cadence logic — "write if offset changed, or if coarse interval elapsed" — so it is the correct method to reuse here. No new DB functions are needed. The key insight is simply that the write responsibility for idle-group partitions cannot live inside _process_group (which is active-group-only by design) and must instead be an explicit step in the run loop.
---


## 2026-02-22 18:03 - TASK 33, 34 - Sampler fixes
- TASK 33: Fixed _evaluate_state_machine to not set any_advancing=True when history is sparse (insufficient data != advancing). This was causing false RECOVERING transitions.
- TASK 34: Fixed _handle_idle_group to preserve last_advancing_at from existing DB state instead of setting to current_time when marking groups as OFFLINE
- Files changed: src/database.py, src/sampler.py, src/tests/test_database.py, src/tests/test_sampler.py
- **Learnings for future iterations:**
  - When fixing state machine bugs, tests may be testing buggy behavior - need to update tests to reflect correct behavior
  - Added get_all_groups_with_history() function to database.py for finding idle groups with history
  - The state machine is conservative: requires positive evidence of advancement, not absence of evidence of stasis
---

## 2026-02-22 19:30 - TASK 35 - Batch commits in sampler write path
- Removed conn.commit() from insert_partition_offset and insert_consumer_commit in database.py
- Added commit_batch() function to database.py for explicit batch commits
- Added commit_batch() call at end of sampler cycle in sampler.py
- Updated tests in test_sampler.py, test_housekeeping.py, test_reporter.py to call commit_batch after inserts
- Files changed: src/database.py, src/sampler.py, src/tests/test_sampler.py, src/tests/test_housekeeping.py, src/tests/test_reporter.py
- **Learnings for future iterations:**
  - When removing auto-commit from insert functions, all callers (tests and code) need to explicitly call commit_batch
  - Tests using worker threads need to use the worker's connection (e.g., s._db_conn) not the fixture connection for queries after operations
  - WAL mode should allow visibility across connections, but explicit commit is still needed for durability
---

## TASK 36 - Introduces a new bug: connection-per-write is a resource leak and correctness risk
Status: Lock scope reduced correctly. But the implementation is problematic.
The lock scope fix is right — in-memory update under lock, DB write outside. However, the chosen approach creates a brand new sqlite3.Connection on every call to set_group_status, then closes it after the single write.
Problem 1 — Connection churn: set_group_status is called once per group per topic per sampler cycle — potentially dozens of times per 30-second cycle. Each call opens a new file descriptor, then database.get_connection runs _create_connection, which executes and commits three PRAGMAs (auto_vacuum, journal_mode, synchronous) before the actual row write. That's two commits (PRAGMAs + row) and a close, per call, per cycle. For 10 groups × 5 topics that's 100 connection open/close operations and 200 commits every 30 seconds, just for status persistence.
Problem 2 — self._db_conn is now a permanently open idle connection: It is created in __init__, used only in _load_group_statuses at startup, then never touched again. It holds a file descriptor and a WAL reader slot for the entire process lifetime. This is pure waste and will confuse any future debugging of connection counts.
Problem 3 — The PRAGMA overhead is especially wasteful: journal_mode=WAL is a database-file property — once set it persists. Re-asserting it on every throwaway connection is redundant. Same with auto_vacuum=INCREMENTAL. These PRAGMAs are correct to set on a long-lived connection at startup; they are noise on a write-and-discard connection.
How to fix:
The original self._db_conn should be retained and repurposed as the persistent write connection for set_group_status. The correct fix is structurally simple: move the upsert_group_status call to execute on self._db_conn but after the with self._lock block closes. The lock guards only the in-memory dict mutation — a pure Python operation that takes microseconds. The DB write then happens outside the lock on the already-open, already-configured persistent connection.
This is safe because set_group_status is only called from two places: the sampler thread (via _evaluate_state_machine and _handle_idle_group) and — in theory — any future caller. Both write paths go through the same StateManager instance. The sampler is single-threaded within its own loop, so there is no concurrent access to self._db_conn from within StateManager's write path. The reporter only calls get_group_status and get_last_json_output, which read in-memory state under the lock and never touch any DB connection on StateManager.
The one-line docstring that says "Updates both in-memory state and persists to database atomically within the same lock acquisition" will also need updating — that atomicity guarantee was already broken by the current implementation and should be removed from the contract description.

## 2026-02-23 03:45 - TASK 36 (FIX) - StateManager connection leak fixed
- Fixed the bug introduced by TASK 36 where set_group_status created a new connection per call
- Restored use of persistent self._db_conn for all database writes
- Added self._db_write_lock to ensure thread-safe access to the connection
- Lock scope now properly separated: self._lock for in-memory state (microseconds), self._db_write_lock for DB writes (milliseconds)
- Files changed: src/state_manager.py
- All 128 tests pass, including concurrent access test
- **Learnings for future iterations:**
  - SQLite connections cannot be shared across threads - need separate lock for connection access
  - Use two locks: one for in-memory state (RLock for re-entrant), one for DB connection (Lock for serialization)
  - In-memory state update is fast (dict operation), DB write is slow (I/O) - keep them separate under different locks
  - The persistent connection pattern eliminates 100+ connection open/close operations and 200+ PRAGMA commits per cycle
  - StateManager is called only from sampler thread in production, but tests validate thread-safety for robustness
---
